{
  "name": "abacus",
  "displayName": "Abacus",
  "description": "Utilizing the Abacus large model to assist Python programming",
  "version": "0.0.7",
  "publisher": "HIT-SCIR",
  "icon": "small_logo.png",
  "engines": {
    "vscode": "^1.82.0"
  },
  "galleryBanner": {
    "color": "#100f11",
    "theme": "dark"
  },
  "license": "Apache-2.0",
  "categories": [
    "Machine Learning",
    "Programming Languages"
  ],
  "keywords": [
    "code",
    "assistant",
    "ai",
    "llm",
    "development"
  ],
  "activationEvents": [
    "*"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "llm.afterInsert",
        "title": "Llm: After Insert"
      },
      {
        "command": "llm.login",
        "title": "Llm: Login"
      },
      {
        "command": "llm.logout",
        "title": "Llm: Logout"
      },
      {
        "command": "llm.attribution",
        "title": "Llm: Show Code Attribution"
      }
    ],
    "configuration": [
      {
        "title": "Llm",
        "properties": {
          "llm.requestDelay": {
            "type": "integer",
            "default": 150,
            "description": "Delay between requests in milliseconds"
          },
          "llm.enableAutoSuggest": {
            "type": "boolean",
            "default": true,
            "description": "Enable automatic suggestions"
          },
          "llm.requestBody": {
            "type": "object",
            "default": {
              "parameters": {
                "max_new_tokens": 128,
                "temperature": 0.2,
                "top_p": 0.95
              }
            },
            "description": "Whatever you set here will be sent as is as the HTTP POST request body to the chosen backend. Model and prompt will be added automatically."
          },
          "llm.contextWindow": {
            "type": "integer",
            "default": 2048,
            "description": "Context window of the model"
          },
          "llm.tokensToClear": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "default": [
              "<|endoftext|>"
            ],
            "description": "(Optional) Tokens that should be cleared from the resulting output. For example, in FIM mode, one usually wants to clear FIM token from resulting outout."
          },
          "llm.attributionWindowSize": {
            "type": "integer",
            "default": 250,
            "description": "Number of characters to scan for code attribution"
          },
          "llm.attributionEndpoint": {
            "type": "string",
            "default": "https://stack.dataportraits.org/overlap",
            "description": "Endpoint to which attribution request will be sent to (https://stack.dataportraits.org/overlap for the stack)"
          },
          "llm.tlsSkipVerifyInsecure": {
            "type": "boolean",
            "default": false,
            "description": "Skip TLS verification for insecure connections"
          },
          "llm.lsp.binaryPath": {
            "type": [
              "string",
              "null"
            ],
            "default": null,
            "description": "Path to llm-ls binary, useful for debugging or when building from source"
          },
          "llm.lsp.port": {
            "type": [
              "number",
              "null"
            ],
            "default": null,
            "description": "When running llm-ls with `--port`, port for the llm-ls server"
          },
          "llm.lsp.logLevel": {
            "type": "string",
            "default": "warn",
            "description": "llm-ls log level"
          },
          "llm.tokenizer": {
            "type": [
              "object",
              "null"
            ],
            "default": null,
            "description": "Tokenizer configuration for the model, check out the documentation for more details"
          },
          "llm.documentFilter": {
            "type": [
              "object",
              "array"
            ],
            "default": {
              "pattern": "**/*.py"
            },
            "description": "Filter documents to enable suggestions for"
          }
        }
      }
    ],
    "keybindings": [
      {
        "key": "alt+shift+l",
        "command": "editor.action.inlineSuggest.trigger"
      },
      {
        "key": "cmd+shift+a",
        "command": "llm.attribution"
      }
    ]
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "pretest": "npm run compile",
    "test": "node ./out/test/runTest.js"
  },
  "dependencies": {
    "undici": "^6.6.2",
    "vscode-languageclient": "^9.0.1"
  },
  "devDependencies": {
    "@types/mocha": "^10.0.6",
    "@types/node": "16.x",
    "@types/vscode": "^1.82.0",
    "@typescript-eslint/eslint-plugin": "^6.21.0",
    "@typescript-eslint/parser": "^6.21.0",
    "@vscode/test-electron": "^2.3.9",
    "@vscode/vsce": "^2.23.0",
    "eslint": "^8.56.0",
    "glob": "^10.3.10",
    "mocha": "^10.3.0",
    "ovsx": "^0.8.3",
    "typescript": "^5.3.3"
  }
}
